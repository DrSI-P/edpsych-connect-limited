/**
 * üö® CRITICAL AI INFRASTRUCTURE
 * Core AI Service for EdPsych Connect Invisible Intelligence Layer
 *
 * This is the foundation of the revolutionary AI brain that makes EdPsych Connect
 * unique. Every interaction demonstrates genuine intelligence and invisible technology.
 */

import Anthropic from '@anthropic-ai/sdk';
import OpenAI from 'openai';
import crypto from 'crypto';

interface AIResponse {
  content: string;
  model: string;
  cost: number;
  tokens: number;
  fromCache: boolean;
  responseTime: number;
}

interface AIRequest {
  prompt: string;
  context?: any;
  id: string;
  subscriptionId: string; // Added missing property
  subscriptionTier: string;
  useCase: string;
  maxTokens?: number;
  temperature?: number;
}

interface CacheEntry {
  content: string;
  model: string;
  cost: number;
  tokens: number;
  createdAt: Date;
  accessCount: number;
}

// Define response usage type
interface ResponseUsage {
  input_tokens?: number;
  output_tokens?: number;
  total_tokens?: number;
}

// Define API response interface
interface AIProviderResponse {
  content: string;
  usage: number;
}

export class AIService {
  private anthropic: Anthropic;
  private openai: OpenAI;
  private cache: Map<string, CacheEntry>;
  private dailyUsage: Map<string, number>;
  private dailyBudget: number;
  private totalCost: number = 0;

  constructor() {
    // Initialize Anthropic Claude (Primary)
    this.anthropic = new Anthropic({
      apiKey: process.env.ANTHROPIC_API_KEY,
    });

    // Initialize OpenAI (Fallback)
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });

    // Initialize in-memory cache (Redis in production)
    this.cache = new Map();

    // Initialize usage tracking
    this.dailyUsage = new Map();
    this.dailyBudget = parseFloat(process.env.AI_DAILY_BUDGET_USD || '50');

    console.log('ü§ñ AI Service initialized with Anthropic Claude + OpenAI fallback');
  }

  /**
   * Generate AI response with intelligent caching, cost tracking, and fallback
   */
  async generateResponse(params: AIRequest): Promise<AIResponse> {
    const startTime = Date.now();

    try {
      // Check rate limits
      if (!this.checkRateLimit(params.subscriptionId, params.subscriptionTier)) {
        throw new Error('Daily AI request limit exceeded. Upgrade your plan for more requests.');
      }

      // Check cache first
      const cacheKey = this.generateCacheKey(params);
      const cached = this.getFromCache(cacheKey);

      if (cached && this.shouldUseCache(params.subscriptionTier)) {
        console.log(`‚úÖ Cache hit for ${params.useCase} - saved API call`);
        return {
          ...cached,
          fromCache: true,
          responseTime: Date.now() - startTime
        };
      }

      // Select appropriate model
      const model = this.selectModel(params.useCase, params.subscriptionTier);

      // Check budget before making API call
      const estimatedCost = this.estimateCost(params.prompt, model);
      if (!this.checkBudget(estimatedCost)) {
        throw new Error('Daily AI budget exceeded. Please try again tomorrow.');
      }

      // Generate response
      const response = await this.callAI(model, params);

      // Calculate actual cost
      const actualCost = this.calculateCost(response.usage, model);

      // Track usage
      this.trackUsage(params.subscriptionId, actualCost);
      this.totalCost += actualCost;

      // Cache the response
      if (this.shouldCache(params.useCase)) {
        this.setCache(cacheKey, {
          content: response.content,
          model,
          cost: actualCost,
          tokens: response.usage,
          createdAt: new Date(),
          accessCount: 1
        });
      }

      console.log(`‚úÖ AI Response generated: ${params.useCase} | Model: ${model} | Cost: ¬£${actualCost.toFixed(4)}`);

      return {
        content: response.content,
        model,
        cost: actualCost,
        tokens: response.usage,
        fromCache: false,
        responseTime: Date.now() - startTime
      };

    } catch (error) {
      console.error('‚ùå AI Service error:', error);
      return this.handleFailover(params, error, startTime);
    }
  }

  /**
   * Intelligent model selection based on use case and subscription tier
   */
  private selectModel(useCase: string, tier: string): string {
    const modelMatrix: Record<string, string> = {
      // Simple, fast tasks
      'simple_template': 'claude-3-haiku-20240307',
      'basic_question': 'claude-3-haiku-20240307',
      'caching': 'claude-3-haiku-20240307',

      // Standard educational tasks
      'report_writing': tier === 'enterprise' ? 'claude-3-opus-20240229' : 'claude-3-sonnet-20240229',
      'lesson_planning': 'claude-3-sonnet-20240229',
      'parent_communication': 'claude-3-sonnet-20240229',
      'assessment_generation': 'claude-3-sonnet-20240229',
      'behavior_analysis': 'claude-3-sonnet-20240229',

      // Complex, creative tasks
      'creative_content': 'claude-3-opus-20240229',
      'research_analysis': 'claude-3-opus-20240229',
      'predictive_modeling': 'claude-3-opus-20240229',
      'curriculum_design': 'claude-3-opus-20240229',

      // Problem matching and adaptation
      'problem_matching': 'claude-3-sonnet-20240229',
      'navigation_optimization': 'claude-3-sonnet-20240229',
      'conversion_optimization': 'claude-3-sonnet-20240229',
      'personalization': 'claude-3-sonnet-20240229'
    };

    return modelMatrix[useCase] || 'claude-3-haiku-20240307';
  }

  /**
   * Call AI provider with error handling and retry logic
   */
  private async callAI(model: string, params: AIRequest): Promise<AIProviderResponse> {
    const maxRetries = 3;
    let lastError: Error | null = null;

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        if (model.includes('claude')) {
          return await this.callAnthropic(model, params);
        } else {
          return await this.callOpenAI(model, params);
        }
      } catch (error) {
        lastError = error as Error;
        console.warn(`‚ö†Ô∏è AI call attempt ${attempt} failed:`, error);

        if (attempt < maxRetries) {
          // Exponential backoff
          await new Promise(resolve => setTimeout(resolve, Math.pow(2, attempt) * 1000));
        }
      }
    }

    throw lastError || new Error('AI service unavailable');
  }

  /**
   * Call Anthropic Claude API
   */
  private async callAnthropic(model: string, params: AIRequest): Promise<AIProviderResponse> {
    const response = await this.anthropic.messages.create({
      model,
      max_tokens: params.maxTokens || 1000,
      temperature: params.temperature || 0.7,
      messages: [
        {
          role: 'user',
          content: this.buildPrompt(params)
        }
      ]
    });

    // Handle different content block types
    const contentBlock = response.content.find(block => block.type === 'text');
    if (!contentBlock || contentBlock.type !== 'text') {
      throw new Error('Unexpected response format from Anthropic API');
    }

    return {
      content: contentBlock.text,
      usage: (response.usage?.input_tokens || 0) + (response.usage?.output_tokens || 0)
    };
  }

  /**
   * Call OpenAI API (fallback)
   */
  private async callOpenAI(model: string, params: AIRequest): Promise<AIProviderResponse> {
    const response = await this.openai.chat.completions.create({
      model: model.includes('gpt') ? model : 'gpt-3.5-turbo',
      messages: [
        {
          role: 'user',
          content: this.buildPrompt(params)
        }
      ],
      max_tokens: params.maxTokens || 1000,
      temperature: params.temperature || 0.7
    });

    return {
      content: response.choices[0].message.content || '',
      usage: response.usage?.total_tokens || 0
    };
  }

  /**
   * Build intelligent prompt with context
   */
  private buildPrompt(params: AIRequest): string {
    let prompt = params.prompt;

    // Add context if provided
    if (params.context) {
      prompt = `Context: ${JSON.stringify(params.context)}\n\n${prompt}`;
    }

    // Add use case specific instructions
    const useCaseInstructions = this.getUseCaseInstructions(params.useCase);
    if (useCaseInstructions) {
      prompt = `${useCaseInstructions}\n\n${prompt}`;
    }

    return prompt;
  }

  /**
   * Get use case specific instructions for better AI responses
   */
  private getUseCaseInstructions(useCase: string): string {
    const instructions: Record<string, string> = {
      'report_writing': 'You are an experienced educational psychologist writing professional, sensitive, and constructive student reports. Focus on progress, provide actionable next steps, and maintain an encouraging tone.',

      'lesson_planning': 'You are a creative and experienced teacher designing engaging, differentiated lesson plans aligned with UK curriculum standards. Include multiple intelligence pathways and assessment for learning strategies.',

      'parent_communication': 'You are a skilled educator communicating with parents. Be specific but sensitive, focus on partnership and support, and provide clear next steps.',

      'behavior_analysis': 'You are an educational psychologist analyzing student behavior patterns. Provide evidence-based insights, identify underlying causes, and recommend supportive interventions.',

      'problem_matching': 'You are an AI assistant helping educators identify solutions to their challenges. Analyze the problem, categorize it, and recommend specific platform features that address it.',

      'navigation_optimization': 'You are optimizing the user experience for an educational platform. Analyze user behavior and recommend navigation improvements.',

      'conversion_optimization': 'You are optimizing conversion rates for an EdTech platform. Analyze user engagement and recommend improvements to increase sign-ups and engagement.'
    };

    return instructions[useCase] || '';
  }

  /**
   * Generate cache key for request
   */
  private generateCacheKey(params: AIRequest): string {
    const hash = crypto.createHash('sha256');
    hash.update(JSON.stringify({
      prompt: params.prompt,
      useCase: params.useCase,
      context: params.context,
      maxTokens: params.maxTokens,
      temperature: params.temperature
    }));
    return `ai_cache:${hash.digest('hex')}`;
  }

  /**
   * Get from cache
   */
  private getFromCache(key: string): AIResponse | null {
    const cached = this.cache.get(key);
    if (cached) {
      cached.accessCount++;
      return {
        content: cached.content,
        model: cached.model,
        cost: cached.cost,
        tokens: cached.tokens,
        fromCache: true,
        responseTime: 0
      };
    }
    return null;
  }

  /**
   * Set cache entry
   */
  private setCache(key: string, entry: CacheEntry): void {
    this.cache.set(key, entry);
    // In production, this would use Redis with TTL
  }

  /**
   * Check if request should use cache
   */
  private shouldUseCache(tier: string): boolean {
    const cacheUsage: Record<string, number> = {
      'standard': 0.8,      // 80% cache usage
      'professional': 0.6,  // 60% cache usage
      'enterprise': 0.4,    // 40% cache usage
      'research': 0.7       // 70% cache usage
    };
    return Math.random() < (cacheUsage[tier] || 0.8);
  }

  /**
   * Check if use case should be cached
   */
  private shouldCache(useCase: string): boolean {
    const cacheableUseCases = [
      'simple_template',
      'basic_question',
      'lesson_planning',
      'report_writing',
      'parent_communication'
    ];
    return cacheableUseCases.includes(useCase);
  }

  /**
   * Check rate limits
   */
  private checkRateLimit(id: string, tier: string): boolean {
    const limits: Record<string, number> = {
      'standard': 50,
      'professional': 500,
      'enterprise': 9999,
      'research': 200
    };

    const limit = limits[tier] || 50;
    const usage = this.dailyUsage.get(id) || 0;

    return usage < limit;
  }

  /**
   * Check budget before API call
   */
  private checkBudget(estimatedCost: number): boolean {
    return (this.totalCost + estimatedCost) <= this.dailyBudget;
  }

  /**
   * Estimate cost before API call
   */
  private estimateCost(prompt: string, model: string): number {
    const inputTokens = Math.ceil(prompt.length / 4); // Rough token estimation
    const estimatedOutputTokens = 500; // Conservative estimate

    const costs: Record<string, { input: number; output: number }> = {
      'claude-3-haiku-20240307': { input: 0.00025, output: 0.00125 },
      'claude-3-sonnet-20240229': { input: 0.003, output: 0.015 },
      'claude-3-opus-20240229': { input: 0.015, output: 0.075 },
      'gpt-3.5-turbo': { input: 0.0015, output: 0.002 },
      'gpt-4-turbo': { input: 0.01, output: 0.03 }
    };

    const cost = costs[model] || costs['claude-3-haiku-20240307'];
    return (inputTokens * cost.input + estimatedOutputTokens * cost.output) / 1000; // Convert to GBP
  }

  /**
   * Calculate actual cost after API call
   */
  private calculateCost(tokens: number, model: string): number {
    const costs: Record<string, number> = {
      'claude-3-haiku-20240307': 0.00125,
      'claude-3-sonnet-20240229': 0.015,
      'claude-3-opus-20240229': 0.075,
      'gpt-3.5-turbo': 0.002,
      'gpt-4-turbo': 0.03
    };

    const costPerThousand = costs[model] || 0.00125;
    return (tokens * costPerThousand) / 1000; // Convert to GBP
  }

  /**
   * Track usage
   */
  private trackUsage(id: string, cost: number): void {
    const currentUsage = this.dailyUsage.get(id) || 0;
    this.dailyUsage.set(id, currentUsage + 1);

    // In production, this would store in database
    console.log(`üìä AI Usage: User ${id} | Cost: ¬£${cost.toFixed(4)} | Total Today: ¬£${(this.totalCost + cost).toFixed(4)}`);
  }

  /**
   * Handle failover to alternative provider
   */
  private async handleFailover(params: AIRequest, error: any, startTime: number): Promise<AIResponse> {
    console.warn('‚ö†Ô∏è Primary AI provider failed, attempting failover:', error.message);

    try {
      // Try alternative provider
      const fallbackModel = this.getFallbackModel(params.useCase);
      const response = await this.callAI(fallbackModel, params);
      const cost = this.calculateCost(response.usage, fallbackModel);

      this.trackUsage(params.subscriptionId, cost);

      return {
        content: response.content,
        model: fallbackModel,
        cost,
        tokens: response.usage,
        fromCache: false,
        responseTime: Date.now() - startTime
      };
    } catch (fallbackError) {
      console.error('‚ùå Both AI providers failed:', fallbackError);

      // Return cached response if available
      const cacheKey = this.generateCacheKey(params);
      const cached = this.getFromCache(cacheKey);

      if (cached) {
        console.log('‚úÖ Using cached response as last resort');
        return {
          ...cached,
          fromCache: true,
          responseTime: Date.now() - startTime
        };
      }

      throw new Error('AI service temporarily unavailable. Please try again later.');
    }
  }

  /**
   * Get fallback model for use case
   */
  private getFallbackModel(useCase: string): string {
    const fallbacks: Record<string, string> = {
      'claude-3-haiku-20240307': 'gpt-3.5-turbo',
      'claude-3-sonnet-20240229': 'gpt-4-turbo',
      'claude-3-opus-20240229': 'gpt-4-turbo',
      'gpt-3.5-turbo': 'claude-3-haiku-20240307',
      'gpt-4-turbo': 'claude-3-sonnet-20240229'
    };

    const primaryModel = this.selectModel(useCase, 'enterprise'); // Use highest tier for fallback
    return fallbacks[primaryModel] || 'claude-3-haiku-20240307';
  }

  /**
   * Get usage statistics
   */
  getUsageStats(): Record<string, number | string> {
    return {
      totalCost: this.totalCost,
      dailyBudget: this.dailyBudget,
      cacheSize: this.cache.size,
      uniqueUsers: this.dailyUsage.size,
      budgetUtilization: (this.totalCost / this.dailyBudget) * 100
    };
  }
}

// Export singleton instance
export const aiService = new AIService();